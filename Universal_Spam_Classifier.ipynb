{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# 💌 通用垃圾訊息分類器 | Universal Spam Classifier (SMS & Email)\n",
        "\n",
        "歡迎來到通用垃圾訊息分類器的實作筆記本！\n",
        "\n",
        "這份 Colab 筆記本將帶您打造一個強大的**通用垃圾訊息分類器**，它不僅能處理簡訊 📱，也能處理電子郵件 📧！\n",
        "\n",
        "我們將展示從資料探索、文字雲視覺化，到模型訓練與建立一個即時預測系統的完整流程。 ✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 🛠️ Setup: 載入必要的函式庫\n",
        "\n",
        "首先，我們需要載入所有會用到的 Python 函式庫，包含資料處理、視覺化和機器學習等工具。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Scikit-learn 相關工具\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 下載 NLTK 的停用詞資料庫\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"✅ Setup Complete: 所有函式庫都已準備就緒！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_header"
      },
      "source": [
        "# Part 1: 簡訊垃圾訊息分類 (SMS Spam Classification) 💬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_loading"
      },
      "source": [
        "### 1.1. 資料載入與清理\n",
        "我們將從 Kaggle 的公開資料集 `spam.csv` 開始。這個步驟包含讀取資料、整理欄位、將標籤轉換為數字格式（`ham` -> 0, `spam` -> 1），並移除重複的訊息。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part1_loading_code"
      },
      "source": [
        "# 從 GitHub raw URL 讀取資料\n",
        "url = 'https://raw.githubusercontent.com/vvchung/Universal_Spam_Classifier/master/spam.csv'\n",
        "df = pd.read_csv(url, encoding='latin1')\n",
        "\n",
        "# --- 資料清理 ---\n",
        "# 1. 只保留需要的欄位\n",
        "df = df[['v1', 'v2']]\n",
        "# 2. 重新命名欄位，使其更具可讀性\n",
        "df.rename(columns={'v1': 'target', 'v2': 'text'}, inplace=True)\n",
        "# 3. 將文字標籤轉換為數字\n",
        "df['target'] = LabelEncoder().fit_transform(df['target'])\n",
        "# 4. 移除重複的資料\n",
        "df = df.drop_duplicates(keep='first')\n",
        "\n",
        "print(f\"資料集清理完成！目前資料形狀: {df.shape}\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_eda"
      },
      "source": [
        "### 1.2. 探索性資料分析 (EDA)\n",
        "在建立模型之前，讓我們先來深入了解資料！我們會分析正常訊息和垃圾訊息的比例，並透過漂亮的文字雲 ☁️ 來看看兩者最常用的詞彙有什麼不同。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part1_eda_code"
      },
      "source": [
        "# 1. 查看類別分佈 (Class Distribution)\n",
        "plt.figure(figsize=(6, 6))\n",
        "df['target'].value_counts().plot(kind='pie', autopct='%1.1f%%', labels=['Ham (正常)', 'Spam (垃圾)'], colors=['skyblue', 'salmon'])\n",
        "plt.title('訊息類別分佈')\n",
        "plt.ylabel('') # 隱藏 y 軸標籤\n",
        "plt.show()\n",
        "\n",
        "# 2. 建立文字預處理函式\n",
        "def preprocess_text(text):\n",
        "    # 轉為小寫\n",
        "    text = text.lower()\n",
        "    # 移除標點符號\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    # 分詞並移除停用詞\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# 3. 產生文字雲 (Word Clouds)\n",
        "wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white')\n",
        "\n",
        "# 垃圾訊息文字雲\n",
        "spam_corpus = ' '.join(df[df['target'] == 1]['processed_text'].tolist())\n",
        "spam_wc = wc.generate(spam_corpus)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(spam_wc)\n",
        "plt.title('垃圾訊息 (Spam) 中最常見的詞彙 ☁️')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# 正常訊息文字雲\n",
        "ham_corpus = ' '.join(df[df['target'] == 0]['processed_text'].tolist())\n",
        "ham_wc = wc.generate(ham_corpus)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(ham_wc)\n",
        "plt.title('正常訊息 (Ham) 中最常見的詞彙 ☁️')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_modeling"
      },
      "source": [
        "### 1.3. 模型建立與評估\n",
        "現在，我們要將處理過的文字轉換成機器可以理解的數字格式（向量化），然後訓練一個`多項式樸素貝氏 (Multinomial Naive Bayes)` 分類器，這是一個非常適合處理文字分類問題的經典模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part1_modeling_code"
      },
      "source": [
        "# 定義特徵 (X) 和目標 (y)\n",
        "X = df['processed_text']\n",
        "y = df['target']\n",
        "\n",
        "# 將資料集分割為訓練集和測試集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 使用 CountVectorizer 將文字轉換為詞頻向量\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# 訓練模型\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_counts, y_train)\n",
        "\n",
        "# 在測試集上進行預測與評估\n",
        "y_pred = model.predict(X_test_counts)\n",
        "print(\"--- 💬 SMS 分類器成果 ---\")\n",
        "print(f\"模型準確率 (Accuracy): {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "\n",
        "# 繪製混淆矩陣 (Confusion Matrix)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title('SMS 分類器混淆矩陣')\n",
        "plt.xlabel('預測標籤')\n",
        "plt.ylabel('真實標籤')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "# Part 2: 郵件垃圾訊息分類 (Email Spam Classification) 📂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_loading"
      },
      "source": [
        "### 2.1. 載入 Email 資料集\n",
        "在這個部分，我們將展示如何處理另一種常見的資料格式：由大量獨立文字檔組成的資料集。我們將下載 Ling-Spam 語料庫並編寫一個函式來讀取它。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part2_loading_code"
      },
      "source": [
        "# 下載並解壓縮 Ling-Spam 資料集\n",
        "!wget https://raw.githubusercontent.com/vvchung/Universal_Spam_Classifier/main/lingspam_public.tar.gz -O lingspam_public.tar.gz\n",
        "!tar -xzf lingspam_public.tar.gz\n",
        "\n",
        "def load_email_data(base_path, folder_indices):\n",
        "    texts, labels = [], []\n",
        "    for i in folder_indices:\n",
        "        folder_path = os.path.join(base_path, f'part{i}')\n",
        "        # 確保資料夾存在\n",
        "        if not os.path.exists(folder_path):\n",
        "            continue\n",
        "        for filename in os.listdir(folder_path):\n",
        "            label = 1 if filename.startswith('spmsg') else 0\n",
        "            with open(os.path.join(folder_path, filename), 'r', errors='ignore') as f:\n",
        "                texts.append(f.read())\n",
        "                labels.append(label)\n",
        "    return texts, labels\n",
        "\n",
        "# 定義訓練集和測試集的路徑（9 個資料夾用於訓練，1 個用於測試）\n",
        "DATA_DIR = 'lingspam_public/bare' # 使用最原始的文字版本\n",
        "email_X_train, email_y_train = load_email_data(DATA_DIR, list(range(1, 10)))\n",
        "email_X_test, email_y_test = load_email_data(DATA_DIR, [10])\n",
        "\n",
        "print(f\"📧 Email 資料載入完成！\")\n",
        "print(f\"訓練資料筆數: {len(email_X_train)}\")\n",
        "print(f\"測試資料筆數: {len(email_X_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_modeling"
      },
      "source": [
        "### 2.2. 訓練 Email 分類器\n",
        "對於 Email 資料，我們使用 `TfidfVectorizer`。與 `CountVectorizer` 不同，TF-IDF 不僅考慮詞頻，還會考慮一個詞在所有文件中的普遍程度，這對於較長的文件（如郵件）通常效果更好。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part2_modeling_code"
      },
      "source": [
        "# 使用 TfidfVectorizer 將 Email 文字轉換為 TF-IDF 向量\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "email_X_train_tfidf = tfidf_vectorizer.fit_transform(email_X_train)\n",
        "email_X_test_tfidf = tfidf_vectorizer.transform(email_X_test)\n",
        "\n",
        "# 訓練一個新的 Naive Bayes 模型\n",
        "email_model = MultinomialNB()\n",
        "email_model.fit(email_X_train_tfidf, email_y_train)\n",
        "email_y_pred = email_model.predict(email_X_test_tfidf)\n",
        "\n",
        "print(\"--- 📂 Email 分類器成果 ---\")\n",
        "print(f\"模型準確率 (Accuracy): {accuracy_score(email_y_test, email_y_pred) * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_header"
      },
      "source": [
        "# Part 3: 終極預測系統 (Unified Predictive System) 🎯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_description"
      },
      "source": [
        "最後，我們來打造一個真正實用的系統！我們將使用在 Part 1 中訓練好的 SMS 模型 (`model`) 和向量化工具 (`vectorizer`)，因為它的預處理流程最完整。這個函式可以接收任何文字輸入，並立即判斷它是否為垃圾訊息。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part3_predictive_code"
      },
      "source": [
        "def predict_message(text):\n",
        "    \"\"\"\n",
        "    使用我們訓練好的 SMS 模型來預測任何新訊息。\n",
        "    Args:\n",
        "        text (str): 輸入的訊息 (SMS 或 Email)。\n",
        "    Returns:\n",
        "        str: 預測結果，'Spam (垃圾)' 或 'Not Spam (Ham) (正常)'。\n",
        "    \"\"\"\n",
        "    # 1. 使用與訓練時相同的流程預處理文字\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # 2. 使用已訓練好的 vectorizer 轉換文字\n",
        "    text_counts = vectorizer.transform([processed_text])\n",
        "\n",
        "    # 3. 使用已訓練好的 model 進行預測\n",
        "    prediction = model.predict(text_counts)[0]\n",
        "\n",
        "    # 4. 回傳人類可讀的結果\n",
        "    if prediction == 1:\n",
        "        return \"Spam (垃圾) 😠\"\n",
        "    else:\n",
        "        return \"Not Spam (Ham) (正常) 😊\"\n",
        "\n",
        "# --- 🚀 實戰測試 ---\n",
        "print(\"--- 終極預測系統實戰 ---\")\n",
        "\n",
        "# 範例 1: 典型的垃圾簡訊\n",
        "sms_spam = 'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)'\n",
        "print(f\"輸入: '{sms_spam}'\")\n",
        "print(f\"預測結果: {predict_message(sms_spam)}\\n\")\n",
        "\n",
        "# 範例 2: 典型的垃圾郵件\n",
        "email_spam = 'Subject: Low-cost Insurance. Are you paying too much for your car insurance? Get a free quote today and save up to 40%. Visit our website now!'\n",
        "print(f\"輸入: '{email_spam}'\")\n",
        "print(f\"預測結果: {predict_message(email_spam)}\\n\")\n",
        "\n",
        "# 範例 3: 正常的訊息\n",
        "ham_message = 'Hi Sarah, just wanted to confirm our meeting for tomorrow at 10 AM. Let me know if that still works for you.'\n",
        "print(f\"輸入: '{ham_message}'\")\n",
        "print(f\"預測結果: {predict_message(ham_message)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}