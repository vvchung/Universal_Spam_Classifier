{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ğŸ’Œ é€šç”¨åƒåœ¾è¨Šæ¯åˆ†é¡å™¨ | Universal Spam Classifier (SMS & Email)\n",
        "\n",
        "æ­¡è¿ä¾†åˆ°é€šç”¨åƒåœ¾è¨Šæ¯åˆ†é¡å™¨çš„å¯¦ä½œç­†è¨˜æœ¬ï¼\n",
        "\n",
        "é€™ä»½ Colab ç­†è¨˜æœ¬å°‡å¸¶æ‚¨æ‰“é€ ä¸€å€‹å¼·å¤§çš„**é€šç”¨åƒåœ¾è¨Šæ¯åˆ†é¡å™¨**ï¼Œå®ƒä¸åƒ…èƒ½è™•ç†ç°¡è¨Š ğŸ“±ï¼Œä¹Ÿèƒ½è™•ç†é›»å­éƒµä»¶ ğŸ“§ï¼\n",
        "\n",
        "æˆ‘å€‘å°‡å±•ç¤ºå¾è³‡æ–™æ¢ç´¢ã€æ–‡å­—é›²è¦–è¦ºåŒ–ï¼Œåˆ°æ¨¡å‹è¨“ç·´èˆ‡å»ºç«‹ä¸€å€‹å³æ™‚é æ¸¬ç³»çµ±çš„å®Œæ•´æµç¨‹ã€‚ âœ¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## ğŸ› ï¸ Setup: è¼‰å…¥å¿…è¦çš„å‡½å¼åº«\n",
        "\n",
        "é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦è¼‰å…¥æ‰€æœ‰æœƒç”¨åˆ°çš„ Python å‡½å¼åº«ï¼ŒåŒ…å«è³‡æ–™è™•ç†ã€è¦–è¦ºåŒ–å’Œæ©Ÿå™¨å­¸ç¿’ç­‰å·¥å…·ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Scikit-learn ç›¸é—œå·¥å…·\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ä¸‹è¼‰ NLTK çš„åœç”¨è©è³‡æ–™åº«\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"âœ… Setup Complete: æ‰€æœ‰å‡½å¼åº«éƒ½å·²æº–å‚™å°±ç·’ï¼\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_header"
      },
      "source": [
        "# Part 1: ç°¡è¨Šåƒåœ¾è¨Šæ¯åˆ†é¡ (SMS Spam Classification) ğŸ’¬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_loading"
      },
      "source": [
        "### 1.1. è³‡æ–™è¼‰å…¥èˆ‡æ¸…ç†\n",
        "æˆ‘å€‘å°‡å¾ Kaggle çš„å…¬é–‹è³‡æ–™é›† `spam.csv` é–‹å§‹ã€‚é€™å€‹æ­¥é©ŸåŒ…å«è®€å–è³‡æ–™ã€æ•´ç†æ¬„ä½ã€å°‡æ¨™ç±¤è½‰æ›ç‚ºæ•¸å­—æ ¼å¼ï¼ˆ`ham` -> 0, `spam` -> 1ï¼‰ï¼Œä¸¦ç§»é™¤é‡è¤‡çš„è¨Šæ¯ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part1_loading_code"
      },
      "source": [
        "# å¾ GitHub raw URL è®€å–è³‡æ–™\n",
        "url = 'https://raw.githubusercontent.com/vvchung/Universal_Spam_Classifier/master/spam.csv'\n",
        "df = pd.read_csv(url, encoding='latin1')\n",
        "\n",
        "# --- è³‡æ–™æ¸…ç† ---\n",
        "# 1. åªä¿ç•™éœ€è¦çš„æ¬„ä½\n",
        "df = df[['v1', 'v2']]\n",
        "# 2. é‡æ–°å‘½åæ¬„ä½ï¼Œä½¿å…¶æ›´å…·å¯è®€æ€§\n",
        "df.rename(columns={'v1': 'target', 'v2': 'text'}, inplace=True)\n",
        "# 3. å°‡æ–‡å­—æ¨™ç±¤è½‰æ›ç‚ºæ•¸å­—\n",
        "df['target'] = LabelEncoder().fit_transform(df['target'])\n",
        "# 4. ç§»é™¤é‡è¤‡çš„è³‡æ–™\n",
        "df = df.drop_duplicates(keep='first')\n",
        "\n",
        "print(f\"è³‡æ–™é›†æ¸…ç†å®Œæˆï¼ç›®å‰è³‡æ–™å½¢ç‹€: {df.shape}\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_eda"
      },
      "source": [
        "### 1.2. æ¢ç´¢æ€§è³‡æ–™åˆ†æ (EDA)\n",
        "åœ¨å»ºç«‹æ¨¡å‹ä¹‹å‰ï¼Œè®“æˆ‘å€‘å…ˆä¾†æ·±å…¥äº†è§£è³‡æ–™ï¼æˆ‘å€‘æœƒåˆ†ææ­£å¸¸è¨Šæ¯å’Œåƒåœ¾è¨Šæ¯çš„æ¯”ä¾‹ï¼Œä¸¦é€éæ¼‚äº®çš„æ–‡å­—é›² â˜ï¸ ä¾†çœ‹çœ‹å…©è€…æœ€å¸¸ç”¨çš„è©å½™æœ‰ä»€éº¼ä¸åŒã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part1_eda_code"
      },
      "source": [
        "# 1. æŸ¥çœ‹é¡åˆ¥åˆ†ä½ˆ (Class Distribution)\n",
        "plt.figure(figsize=(6, 6))\n",
        "df['target'].value_counts().plot(kind='pie', autopct='%1.1f%%', labels=['Ham (æ­£å¸¸)', 'Spam (åƒåœ¾)'], colors=['skyblue', 'salmon'])\n",
        "plt.title('è¨Šæ¯é¡åˆ¥åˆ†ä½ˆ')\n",
        "plt.ylabel('') # éš±è— y è»¸æ¨™ç±¤\n",
        "plt.show()\n",
        "\n",
        "# 2. å»ºç«‹æ–‡å­—é è™•ç†å‡½å¼\n",
        "def preprocess_text(text):\n",
        "    # è½‰ç‚ºå°å¯«\n",
        "    text = text.lower()\n",
        "    # ç§»é™¤æ¨™é»ç¬¦è™Ÿ\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    # åˆ†è©ä¸¦ç§»é™¤åœç”¨è©\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# 3. ç”¢ç”Ÿæ–‡å­—é›² (Word Clouds)\n",
        "wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white')\n",
        "\n",
        "# åƒåœ¾è¨Šæ¯æ–‡å­—é›²\n",
        "spam_corpus = ' '.join(df[df['target'] == 1]['processed_text'].tolist())\n",
        "spam_wc = wc.generate(spam_corpus)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(spam_wc)\n",
        "plt.title('åƒåœ¾è¨Šæ¯ (Spam) ä¸­æœ€å¸¸è¦‹çš„è©å½™ â˜ï¸')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# æ­£å¸¸è¨Šæ¯æ–‡å­—é›²\n",
        "ham_corpus = ' '.join(df[df['target'] == 0]['processed_text'].tolist())\n",
        "ham_wc = wc.generate(ham_corpus)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(ham_wc)\n",
        "plt.title('æ­£å¸¸è¨Šæ¯ (Ham) ä¸­æœ€å¸¸è¦‹çš„è©å½™ â˜ï¸')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_modeling"
      },
      "source": [
        "### 1.3. æ¨¡å‹å»ºç«‹èˆ‡è©•ä¼°\n",
        "ç¾åœ¨ï¼Œæˆ‘å€‘è¦å°‡è™•ç†éçš„æ–‡å­—è½‰æ›æˆæ©Ÿå™¨å¯ä»¥ç†è§£çš„æ•¸å­—æ ¼å¼ï¼ˆå‘é‡åŒ–ï¼‰ï¼Œç„¶å¾Œè¨“ç·´ä¸€å€‹`å¤šé …å¼æ¨¸ç´ è²æ° (Multinomial Naive Bayes)` åˆ†é¡å™¨ï¼Œé€™æ˜¯ä¸€å€‹éå¸¸é©åˆè™•ç†æ–‡å­—åˆ†é¡å•é¡Œçš„ç¶“å…¸æ¨¡å‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part1_modeling_code"
      },
      "source": [
        "# å®šç¾©ç‰¹å¾µ (X) å’Œç›®æ¨™ (y)\n",
        "X = df['processed_text']\n",
        "y = df['target']\n",
        "\n",
        "# å°‡è³‡æ–™é›†åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ä½¿ç”¨ CountVectorizer å°‡æ–‡å­—è½‰æ›ç‚ºè©é »å‘é‡\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# è¨“ç·´æ¨¡å‹\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_counts, y_train)\n",
        "\n",
        "# åœ¨æ¸¬è©¦é›†ä¸Šé€²è¡Œé æ¸¬èˆ‡è©•ä¼°\n",
        "y_pred = model.predict(X_test_counts)\n",
        "print(\"--- ğŸ’¬ SMS åˆ†é¡å™¨æˆæœ ---\")\n",
        "print(f\"æ¨¡å‹æº–ç¢ºç‡ (Accuracy): {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "\n",
        "# ç¹ªè£½æ··æ·†çŸ©é™£ (Confusion Matrix)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title('SMS åˆ†é¡å™¨æ··æ·†çŸ©é™£')\n",
        "plt.xlabel('é æ¸¬æ¨™ç±¤')\n",
        "plt.ylabel('çœŸå¯¦æ¨™ç±¤')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "# Part 2: éƒµä»¶åƒåœ¾è¨Šæ¯åˆ†é¡ (Email Spam Classification) ğŸ“‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_loading"
      },
      "source": [
        "### 2.1. è¼‰å…¥ Email è³‡æ–™é›†\n",
        "åœ¨é€™å€‹éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡å±•ç¤ºå¦‚ä½•è™•ç†å¦ä¸€ç¨®å¸¸è¦‹çš„è³‡æ–™æ ¼å¼ï¼šç”±å¤§é‡ç¨ç«‹æ–‡å­—æª”çµ„æˆçš„è³‡æ–™é›†ã€‚æˆ‘å€‘å°‡ä¸‹è¼‰ Ling-Spam èªæ–™åº«ä¸¦ç·¨å¯«ä¸€å€‹å‡½å¼ä¾†è®€å–å®ƒã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part2_loading_code"
      },
      "source": [
        "# ä¸‹è¼‰ä¸¦è§£å£“ç¸® Ling-Spam è³‡æ–™é›†\n",
        "!wget https://raw.githubusercontent.com/vvchung/Universal_Spam_Classifier/main/lingspam_public.tar.gz -O lingspam_public.tar.gz\n",
        "!tar -xzf lingspam_public.tar.gz\n",
        "\n",
        "def load_email_data(base_path, folder_indices):\n",
        "    texts, labels = [], []\n",
        "    for i in folder_indices:\n",
        "        folder_path = os.path.join(base_path, f'part{i}')\n",
        "        # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨\n",
        "        if not os.path.exists(folder_path):\n",
        "            continue\n",
        "        for filename in os.listdir(folder_path):\n",
        "            label = 1 if filename.startswith('spmsg') else 0\n",
        "            with open(os.path.join(folder_path, filename), 'r', errors='ignore') as f:\n",
        "                texts.append(f.read())\n",
        "                labels.append(label)\n",
        "    return texts, labels\n",
        "\n",
        "# å®šç¾©è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„è·¯å¾‘ï¼ˆ9 å€‹è³‡æ–™å¤¾ç”¨æ–¼è¨“ç·´ï¼Œ1 å€‹ç”¨æ–¼æ¸¬è©¦ï¼‰\n",
        "DATA_DIR = 'lingspam_public/bare' # ä½¿ç”¨æœ€åŸå§‹çš„æ–‡å­—ç‰ˆæœ¬\n",
        "email_X_train, email_y_train = load_email_data(DATA_DIR, list(range(1, 10)))\n",
        "email_X_test, email_y_test = load_email_data(DATA_DIR, [10])\n",
        "\n",
        "print(f\"ğŸ“§ Email è³‡æ–™è¼‰å…¥å®Œæˆï¼\")\n",
        "print(f\"è¨“ç·´è³‡æ–™ç­†æ•¸: {len(email_X_train)}\")\n",
        "print(f\"æ¸¬è©¦è³‡æ–™ç­†æ•¸: {len(email_X_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_modeling"
      },
      "source": [
        "### 2.2. è¨“ç·´ Email åˆ†é¡å™¨\n",
        "å°æ–¼ Email è³‡æ–™ï¼Œæˆ‘å€‘ä½¿ç”¨ `TfidfVectorizer`ã€‚èˆ‡ `CountVectorizer` ä¸åŒï¼ŒTF-IDF ä¸åƒ…è€ƒæ…®è©é »ï¼Œé‚„æœƒè€ƒæ…®ä¸€å€‹è©åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­çš„æ™®éç¨‹åº¦ï¼Œé€™å°æ–¼è¼ƒé•·çš„æ–‡ä»¶ï¼ˆå¦‚éƒµä»¶ï¼‰é€šå¸¸æ•ˆæœæ›´å¥½ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part2_modeling_code"
      },
      "source": [
        "# ä½¿ç”¨ TfidfVectorizer å°‡ Email æ–‡å­—è½‰æ›ç‚º TF-IDF å‘é‡\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "email_X_train_tfidf = tfidf_vectorizer.fit_transform(email_X_train)\n",
        "email_X_test_tfidf = tfidf_vectorizer.transform(email_X_test)\n",
        "\n",
        "# è¨“ç·´ä¸€å€‹æ–°çš„ Naive Bayes æ¨¡å‹\n",
        "email_model = MultinomialNB()\n",
        "email_model.fit(email_X_train_tfidf, email_y_train)\n",
        "email_y_pred = email_model.predict(email_X_test_tfidf)\n",
        "\n",
        "print(\"--- ğŸ“‚ Email åˆ†é¡å™¨æˆæœ ---\")\n",
        "print(f\"æ¨¡å‹æº–ç¢ºç‡ (Accuracy): {accuracy_score(email_y_test, email_y_pred) * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_header"
      },
      "source": [
        "# Part 3: çµ‚æ¥µé æ¸¬ç³»çµ± (Unified Predictive System) ğŸ¯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_description"
      },
      "source": [
        "æœ€å¾Œï¼Œæˆ‘å€‘ä¾†æ‰“é€ ä¸€å€‹çœŸæ­£å¯¦ç”¨çš„ç³»çµ±ï¼æˆ‘å€‘å°‡ä½¿ç”¨åœ¨ Part 1 ä¸­è¨“ç·´å¥½çš„ SMS æ¨¡å‹ (`model`) å’Œå‘é‡åŒ–å·¥å…· (`vectorizer`)ï¼Œå› ç‚ºå®ƒçš„é è™•ç†æµç¨‹æœ€å®Œæ•´ã€‚é€™å€‹å‡½å¼å¯ä»¥æ¥æ”¶ä»»ä½•æ–‡å­—è¼¸å…¥ï¼Œä¸¦ç«‹å³åˆ¤æ–·å®ƒæ˜¯å¦ç‚ºåƒåœ¾è¨Šæ¯ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "part3_predictive_code"
      },
      "source": [
        "def predict_message(text):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨æˆ‘å€‘è¨“ç·´å¥½çš„ SMS æ¨¡å‹ä¾†é æ¸¬ä»»ä½•æ–°è¨Šæ¯ã€‚\n",
        "    Args:\n",
        "        text (str): è¼¸å…¥çš„è¨Šæ¯ (SMS æˆ– Email)ã€‚\n",
        "    Returns:\n",
        "        str: é æ¸¬çµæœï¼Œ'Spam (åƒåœ¾)' æˆ– 'Not Spam (Ham) (æ­£å¸¸)'ã€‚\n",
        "    \"\"\"\n",
        "    # 1. ä½¿ç”¨èˆ‡è¨“ç·´æ™‚ç›¸åŒçš„æµç¨‹é è™•ç†æ–‡å­—\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # 2. ä½¿ç”¨å·²è¨“ç·´å¥½çš„ vectorizer è½‰æ›æ–‡å­—\n",
        "    text_counts = vectorizer.transform([processed_text])\n",
        "\n",
        "    # 3. ä½¿ç”¨å·²è¨“ç·´å¥½çš„ model é€²è¡Œé æ¸¬\n",
        "    prediction = model.predict(text_counts)[0]\n",
        "\n",
        "    # 4. å›å‚³äººé¡å¯è®€çš„çµæœ\n",
        "    if prediction == 1:\n",
        "        return \"Spam (åƒåœ¾) ğŸ˜ \"\n",
        "    else:\n",
        "        return \"Not Spam (Ham) (æ­£å¸¸) ğŸ˜Š\"\n",
        "\n",
        "# --- ğŸš€ å¯¦æˆ°æ¸¬è©¦ ---\n",
        "print(\"--- çµ‚æ¥µé æ¸¬ç³»çµ±å¯¦æˆ° ---\")\n",
        "\n",
        "# ç¯„ä¾‹ 1: å…¸å‹çš„åƒåœ¾ç°¡è¨Š\n",
        "sms_spam = 'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)'\n",
        "print(f\"è¼¸å…¥: '{sms_spam}'\")\n",
        "print(f\"é æ¸¬çµæœ: {predict_message(sms_spam)}\\n\")\n",
        "\n",
        "# ç¯„ä¾‹ 2: å…¸å‹çš„åƒåœ¾éƒµä»¶\n",
        "email_spam = 'Subject: Low-cost Insurance. Are you paying too much for your car insurance? Get a free quote today and save up to 40%. Visit our website now!'\n",
        "print(f\"è¼¸å…¥: '{email_spam}'\")\n",
        "print(f\"é æ¸¬çµæœ: {predict_message(email_spam)}\\n\")\n",
        "\n",
        "# ç¯„ä¾‹ 3: æ­£å¸¸çš„è¨Šæ¯\n",
        "ham_message = 'Hi Sarah, just wanted to confirm our meeting for tomorrow at 10 AM. Let me know if that still works for you.'\n",
        "print(f\"è¼¸å…¥: '{ham_message}'\")\n",
        "print(f\"é æ¸¬çµæœ: {predict_message(ham_message)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}